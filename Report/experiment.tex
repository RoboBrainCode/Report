% !TEX root = main.tex
\section{Experiments}
\subsection{Dataset}
Our dataset consists of 90 natural language instructions of which 60 are a sequence of simple move to instructions and 30 are high level natural language instructions. All these instructions are taken using a \href{http://52.25.65.189:9000/#/getFeedback}{crowd sourcing system} and the ground truth for the natural language instructions are also collected for evaluating the model. The instructions are taken for 16 different environments depicting scenes similar to living room, bedroom and kitchen. \\
The environments are constructed in OpenRAVE with context generated from 6 different human activities depicted by skeletals. The activities considered are walking, watching, interacting, reaching, sitting and working. \\
The instructions involve task description for themes in common house-hold enviromnent like cleaning the room, arranging the guest room and serving coffee and also some simple instructions like move to the bed and then to the table. The dataset contains considerable high level ambiguous instructions like distribute, arrange etc. with variety in verbs.\\
The dataset was divided into 60-30 ratio for training and testing purposes respectively.  

% Please see supplementary for examples from our dataset.

\subsection{Set Up:}
	\subsubsection{Planit with Coactive Feedback:} 
		Planit is a crowd source platform that captures human interaction with the environment and learn their preferences over trajectories. Earlier version of planit was trained using expectation maximization using a database of 2500 trajectories over 112 different environments. In the current setting, we aim to to learn user preferences via coactive learning which requires a sub optimal feedback from the user which is more flexible to obtain and requires fewer number of training examples.

		Dataset consists of 16 different environments with 5 different activities occuring in them namely watching, walking, reaching, interacting, etc. User can view the environment in a simulator and can drag the robot to different positions in the environment. User provided the feedback in the form of a pair ($y,\bar{y}$) whereby $U(y) < U(\bar{y})$. This implies that the robot position $\bar{y}$ is more favourable for the user than the robot position $y$ in any trajectory in the environment. 

		Total 432 feedback was collected from the user across 10 different environment. For evaluation, we considered a set of 6 environments (not in the training set) from the dataset. The trained model was used to rank 112 trajectory sets each containing 7 trajectories to move between two randomly sampled points $A$ and $B$. Original version of planit (as reported in the planit paper (expert system) was assumed as the ground truth for evaluating the trajectories. The nDCG (normalized discounted cumulative gain) of \textbf{0.89} for the above model was observed. This is reasonable since in the earlier version the feedback was expert (labelling segments of 2500 different trajectories) and in the given system we have 400 instances of much weaker feedback.\ref{fig:planitResult} 


		
		% \includegraphics[scale=0.5]{planitResult}
		\begin{figure}[h]
		\includegraphics[width=8cm]{planitResult}
		\centering
		\caption{Cost function learnt via Coactive Feedback}
  		\label{fig:planitResult}
		\end{figure}


		\begin{figure}[h]
		\includegraphics[width=9.5cm]{coactiveVSfeed}
		\centering
		\caption{Coactive Learning aginst number of feedbacks}
  		\label{fig:c1}
		\end{figure}
	\subsubsection{TellMeDave with Coactive Feedback:} 
	TellMeDave is system that grounds Natural Language instructions to sequence of robotic actions by capturing the context from the environment. The original version of Tellmedave was trained with stochastic gradient descent over 4 tasks performed in 5 different environments with 50 NL instructions on each. \\
	As mentioned earlier, we aim to to learn user preferences via coactive learning which requires a sub optimal feedback from the user. Dataset chosen was same as that mentioned in the Dataset section with 60 train points and 30 test points containing high-level and low-level NL desciptions of the tasks. \\
	The end-user gave feedback on each of the datapoints from the training set in the form of detailed descriptions. This feedback was hard to get due to difficulty in detecting the error in the pipeline i.e. whether the mistake lies in lexicon selection or object selection. \\
	Evaluation Metrics. We consider two metrics, IED and END\cite{Misra2014tell}, which measure accuracy based on the action sequence and environment, respectively. The Instruction Edit Distance(IED)\ref{fig:IED} metric improved from 9.98\% to 30.21\% and Environment Edit Distance(END)\ref{fig:END} metric improved from 3.79 \% to 27.09\%.
 
	\begin{figure}[h]
		\includegraphics[width=8cm]{IED}
		\centering
		\caption{IED against different number of feedbacks }
  		\label{fig:IED}
		\end{figure}
		
	\begin{figure}[h]
		\includegraphics[width=8cm]{END}
		\centering
		\caption{END against different number of feedbacks }
  		\label{fig:END}
		\end{figure}

	\subsubsection{Joint system Coactive Feedback:}
	In the given setting we cascade the modules corresponding to the language and trajectory planing in such a way that the robot executes a sequence of instructions to perform a task specified by the natural language instruction. The dataset consists of 90 language instructions across 16 environments of which 60 were used for training and 30 were used for evaluation.

	User are shown trajectories for executing a sequence of instruction in a simulator and the user can stop trajectories and update specific waypoints of the trajectory by dragging the waypoint to a better point thus modifying the entire trajectory for performing the task.

	Thus, for a natural language instruction $x$, user observes trajectory $y$ and provides a slighly improved trajectory $\bar{y}$ for executing the instruction. Such feedback may either correspond to modification in the sequence of instructions output by the language module or updating the trajectory as per user preferences. The feedback is such that ($x,\bar{y}$) captures the intent of the user in a better way than ($x,y$).

	We have devised a heuristic that distributes the given feedback to planit and tellmedave and finds sequence of robot instructions that are consistent with the given trajectory.

	The modified trajectories on these 60 datapoints generated around 50 feedback for tellmedave and around 250 feedback for planit.(multiple waypoints can be obtained from a single trajectory improvement for learning planning parameters).

	The performance of planit was evaluated in a similar fashion as the standalone system by ranking 112 trajectory set each consisting of 7 trajectories.The nDCG (normalized discounted cumulative gain) of around \textbf{0.79} was obtained with respect to the ground truth.




\subsection{Results} %\todo{Dip: please take a detailed pass over this subsection}
We test our algorithm on two tasks, in the first task we evaluate the success of co-active feedback in tuning the robot system and in second task, we evaluate the results from different type of feedback.

\noindent\textit{Task 1: Improvement Due to Co-active feedback}
\textit{Conclusion:} The system is able to improve its accuracy by BLAH on a held-out test set, after a few iterations of co-active feebacks from non-expert users.

\begin{table}
\label{tbl:tsk1}
\caption{Table 1: System improves its end-to-end accuracy after receiving co-active feedback on the final robot behavior}
\centering
\begin{tabular}{|l|l|}
\hline
\textit{System} & \textit{Accuracy} \\
\hline
Sys & BLAH \\
Sys + CF & BLAH \\
Sys + MCF & BLAH \\
\hline
\end{tabular}
\end{table}

\noindent\textit{Task 2: Effects of Different Type of Feedback}
\textit{Conclusion:} Our system based on co-active feedback is able to achieve close performance with system using expert feedback. Further, our system vastly outperforms system based on boolean feedback.

\begin{table}
\label{tbl:tsk2}
\caption{Table 2: Co-active feedback achieves close to the performance with expert feedback}
\centering
\begin{tabular}{|l|l|}
\hline
\textit{System} & \textit{Accuracy} \\
\hline
Sys + boolean & --\\
Sys + CF & -- \\
Sys + Expert & -- \\
\hline
\end{tabular}
\end{table}


%\todo{Dip: we should also experiment with individually tuning the two system AND experiment with different number of feedback until an asymptote is attained}
